\chapter{Literature Review}
\label{chapter:literature}


% TODO:
% Joint 3d face reconstruction and dense alignment with position map regression network

% Extreme 3D Face Reconstruction: Seeing Through Occlusions

This section attempts to provide a comprehensive literature review of
the works relevant to this thesis. This thesis focuses primarily on
the 3D reconstruction of human faces, but includes an extension which
shows our method working on full human bodies. Methods for 3D
reconstruction of the face and body often depend on methods for first
extracting landmarks. As such, an overview of literature relating to
face alignment and human pose estimation is given in this literature
review, providing some context for the works relating to 3D face and
human body reconstruction. Additionally, some initial work on facial
part segmentation turned out to be highly influential in our approach
to the problem of face reconstruction. Hence, towards the end of this
chapter, an overview of work relating to semantic segmentation and
semantic part segmentation is provided.

\section{Face Alignment}

Face alignment the process of detecting or aligning a set of facial
points to a facial image. Face alignment is a prerequisite to many 3D
face reconstruction methods - in fact, methods such
as~\cite{huber2016multiresolution} depend only on facial landmarks,
discarding the image entirely. Our own methods optionally makes use of
these landmarks to assist in the 3D reconstruction process. These are
discussed in detail in Chapters~\ref{chapter:face} and~\ref{chapter:seg}.

Until recently, the highest performing methods for face alignment were
based on a cascaded regression approach. These methods estimate the
location of facial landmarks by applying a sequence of
regressors. Provided to these regressors is typically
SIFT~\cite{lowe2004distinctive} and other hand selected
features. These regressors are applied in a cascaded fashion where the
input to regressor $k$ is the estimate from regressor $k-1$. More
details of such approaches to face alignment can be found
in~\cite{sanchez16,cao2014facewarehouse,xiongsupervised,zhu2015face,tzimiropoulos2015project}.

The remainder of this section will focus on deep learning based
approaches to face alignment. Deep convolutional networks have proven
to be an excellent choice for this kind of problem. One of the primary
reasons for this is due to the way global and local context can be
incorporated to provide both accuracy and precision. A CNN casecade is
presented in~\cite{sun2013deep} to directly produce the landmark
locations. Similarly in~\cite{bulat2016two}, the authors propose a
method for 3D face alignment by first detecting the 2D landmarks using
heatmap regression. If desired, the process can be stopped
here. Optionally, this stack of heatmaps is passed through a second
network, this time for regression, accompanied by the original input
image. The output of this second stage is 68 $(x, y, z)$ landmarks. A
multitask approach is proposed in~\cite{zhang2014facial}, regressing
the location of the facial landmarks while performing facial attribute
classification.

Many CNN based alignment methods estimate the facial landmarks by
regressing a 2D Gaussian on top of each landmark
location~\cite{bulat2016two,bulat2017far,mahpod2018facial,kowalski2017deep,merget2018robust}.
This spatial approach is commonly referred to as heatmap
regression. One such example is the extension to~\cite{bulat2016two}
in~\cite{bulat2017far}, where the second regressor is replaced with a
CNN producing a 3D heatmap representation. A similar heatmap based
approach was used in a multi-stage approach described
in~\cite{kowalski2017deep}, where the landmark detections are refined
after each stage. Between each stage, the current landmark prediction
and some generated features are passed again through the network. At
the beginning of each stage, the inputs are transformed using the
landmarks to make the problem less challenging.

An indirect approach is discussed in~\cite{zhu2016face}, where a 3DMM
is first fitted, from which 3D landmarks are taken. This work is
discussed in further detail in
Section~\ref{chap:literature:sec:face_recon} as it is performing 3D
reconstruction, but for the purposes of face alignment. A similar idea
is proposed in~\cite{jourabloo2016large}, where a dense 3D model is
fit to the face first.

\section{Face Reconstruction}
\label{chap:literature:sec:face_recon}

% 3D face reconstruction is the process of estimating the 3D geometry of
% a face. This is typically from one or more images, but methods relying
% on video, depth data, and landmarks also exist. %% More of an
% %% introduction required here.

The literature on 3D face reconstruction typically falls into several
categories. However, there is often also a lot of interaction or
intersection between multiple techniques in an attempt to garner
better results.

%%%%%%%%%%%%%%% 3DMM
\subsection{3D Morphable Models (3DMM)}
Arguably the most popular approach to estimating the 3D geometry of
the human face is using 3D Morphable Models (3DMM). These models use a
small number of parameters to adjust facial properties such as shape,
pose and expression. There are many methods based on this approach,
including~\cite{jourabloo2016large,huber2016multiresolution,zhu2016face,liu2016joint,tran2018extreme,jiang20183d,jiang2018pose}.

In particular,~\cite{jourabloo2016large} use a cascade of CNNs, which
first regress the camera projection, followed by the pose, followed by
shape and expression. Additionally,~\cite{jourabloo2016large} extract
features from their model fitting which can assist in facial landmark
localisation. A method capable of working with models of different
resolutions is proposed in~\cite{huber2016multiresolution}, along with
several models of different resolution, ranging from roughly 3500 to
30,000 vertices. This method does not use a CNN, instead regressing
parameters for shape and pose using only the landmarks. This is done
by iteratively minimising a cost function which evaluates the fitting.

Constraining the pose, shape and (optionally) appearance into a small
number of parameters has the undesirable side effect of being unable
to capture finer details, such as wrinkles and
bumps. In~\cite{sela2017unrestricted}, a CNN is used for 3DMM fitting,
followed by a refinement step which iteratively improves the finer
details. However, during this iterative refinement, the correspondence
between vertices, as well as the scale and spatial location of the
fitting is lost, which disregards one of the main advantages of 3DMM
based approaches. Another iterative approach attempting to
\textit{also} recover both the 3D geometry and facial landmarks is
described in~\cite{liu2016joint}. For initialisation, the method
passes the image and a mean set of facial landmarks. These landmarks
are gradually refined while learning a 3D to 2D mapping, which updates
the 3D face shape. Unlike our own method (described in
Chapter~\ref{chapter:face}),~\cite{liu2016joint} does not estimate the
facial pose, and as such always produces a frontal reconstruction. In
many cases, the pose is an important component, and as such, a
separate method, likely based on the facial landmarks, would have to
be used to find the transformation for the 3D model.

%%%%%%%%%%%%%%% 3DMM + Synthetic
\subsection{Synthetic Training Data for 3DMM}
The data required for training 3D reconstruction methods typically
consists of an image with a corresponding 3D scan. Such pairs can be
difficult to obtain. \cite{richardson20163d} alleviate this issue by
generating synthetic training data, comprising of renderings of
randomly generated 3DMM with the corresponding mesh. Faces are
rendered under random lighting conditions and pose. An incremental
fitting method is then used during inference. Synthetic data was also
used for training a general purpose 3D reconstruction
method~\cite{li2015joint}. Methods trained using synthetic training
data often fail on real or ``in-the-wild'' images due to unrealistic
renderings or constrained lighting.

A partially synthetic approach to 3D face reconstruction (indirectly
for face alignment) is proposed in~\cite{zhu2016face}. The method used
a single CNN that is iteratively applied to estimate the model
parameters using as input the 2D image and 3D shape and pose
representation from the previous iteration. The network consists of
four convolution layers and two fully connected layers. The first two
convolutions share parameters for memory efficient feature
extraction. This shape and pose representation can best be summarised
as a depth map of the fitting generated by parameters from the
previous iteration. The training data is augmented by artificially
rotating faces between $\{-90, 90\}$ degrees using groundtruth
parameters for a frontal fitting. One of the byproducts of this work
is a large dataset consisting of large pose synthetically rotated
faces, each with a corresponding 3D mesh.

%%%%%%%%%%%%%%% Shape from Shading
\subsection{Shape from Shading}
A method which performs 3D face reconstruction from video is presented
in~\cite{suwajanakorn2014total}, which can also work from a set of
static images. The method combines their proposed 3D flow estimation
algorithm with a Shape from Shading (SfS) method. The method first
estimates the facial pose, which is used to rotate an average face
shape to the correct pose. For each frame, 3D flow updates the mesh,
which both refines the detail, and updates aspects such as expression
and pose. Another Shape from Shading approach is proposed
in~\cite{jiang20183d}, which first aligns a coarse face reconstruction
using 2D landmark detections. This coarse model is then
\textit{corrected} using medium scale features from the
image. Finally, the shape from shading step refines the mesh and
introduces finer details.

%%%%%%%%%%%%%%% Multiview Approaches
\subsection{Multiview Reconstruction and Structure from Motion}

Several methods exist which rely on multiple images to reconstruct the
3D geometry of the face. These again fall into two more
categories. These are know as Multiview~\cite{mayo20093d,dou2018multi}
or Structure from Motion
(SfM)~\cite{dou2018multi,dai2018coarse,Piotraschke_2016_CVPR} based
methods, although again, there is some intersection between the two.

A keypoint based approach is presented in~\cite{mayo20093d}, where
SIFT is first used to extract dense features from each image. These
are then aligned using an Iterative Closest Point (ICP) algorithm to
build a point cloud o a face, from which a surface could be extracted
later. However, due to the simplicity of this approach, and the
limitations of ICP, this method will not account for changes in facial
expression between images. The work of~\cite{dou2018multi} proposes to
first pass each image through a CNN to extract feature maps, which are
then passed to a recurrent neural network (RNN) to regress parameters
for a 3DMM. The result is altered with each frame, making it a
suitable method for videos, where facial expression changes over time.

A two stage (coarse to fine) approach is presented
in~\cite{dai2018coarse}, where first a sparse set of landmarks are
first estimated, which are then used to estimate pose to assist in
fitting a dense set of points to the face to create a coarse
reconstruction. During this coarse reconstruction step, 14 images from
specific poses are extracted, which are used to refine the model and
extract details. Finally, in~\cite{Piotraschke_2016_CVPR}, a method is
proposed which reconstructs individual 3D shapes from multiple single
images of a face. The quality of each shape is evaluated. The best of
these shapes are them combined into a single 3D model. The aim of
doing this is to better handle variations in pose, lighting,
occlusions and facial expressions.

\subsection{Depth Estimation}

Finally, depth estimation - while typically applied to problems such
as room reconstruction, there are a couple of methods which attempt to
reconstruct the face by estimating the
depth~\cite{sun2011depth,sun2013depth}. In~\cite{sun2011depth}, a
constrained Independent Component Analysis (ICA) model is used to
analyse multiple facial images of the same subject, from which the 3D
geometry is obtained. An almost identical method is presented
in~\cite{sun2013depth}, except instead of using an ICA model, a
nonlinear Least-Squares model is used, offering a small performance
improvement over~\cite{sun2011depth}.

%%%%%%%%%%%%%%% Our Method
\subsection{Where does our work fit in?}

Our method for 3D face reconstruction, described in
Chapter~\ref{chapter:face} is very different to the aforementioned
methods. First, our method does not rely on a 3D shape model for
either the pre-processing or ``fitting'' procedure, instead we rely on
a volumetric representation. This avoids the requirement to find
correspondence between vertices, which is a challenging problem in
itself. Second, our method is direct, producing a volumetric
representation of the facial geometry in just a single step. Third,
unlike~\cite{tran2018extreme} and multiview approaches, we do not lose
any pose or scale information, and show potential for detailed
reconstruction, demonstrated in Chapter~\ref{chapter:human} on human
bodies.

\section{Human Pose Estimation}

While this thesis does not focus on pose estimation, in
Chapter~\ref{chapter:human}, we present our method for 3D human body
reconstruction as an extension to our 3D face reconstruction
work. Human pose estimation is optionally used as guidance for this
method in a similar fashion to face alignment being used as guidance
to 3D face reconstruction.

Modern approaches to estimating human pose are based on methods
employing CNNs. These methods generally fall into one of two
categories. The first is to directly regress the coordinates of the
joints using an L2 (or similar) loss, see for
example~\cite{li20143d,park20163d,tekin2016structured,tekin2016direct,zhou2016deep,chen2016synthesizing,ghezelghieh2016learning,toshev2014deeppose,tompson2014joint}. In
particular,~\cite{park20163d} estimates the 3D pose by combining the
2D predictions with image features. An autoencoder is employed
in~\cite{tekin2016structured} to constrain pose to something
plausible. Similarly,~\cite{zhou2016deep} have the same goal but
achieve this by using a kinematic model. Synthetic training data is
used for the full training procedure in~\cite{chen2016synthesizing},
to ensure that the network is trained with accurate data. However,
in~\cite{ghezelghieh2016learning}, they only augment their existing
training set with synthetic data. ~\cite{toshev2014deeppose} propose
an iterative method, in which images are first passed through an
initalisation CNN. The output from this is then combined again with
the input image and passed through a second CNN multiple times
iteratively.  In~\cite{tompson2014joint}, a multi-resolution CNN is
trained jointly with a Markov Random Field, helping to refine the
prediction from the CNN by constraining pose. An iterative method is
presented in~\cite{carreira2016human}. However,
unlike~\cite{toshev2014deeppose}, which directly outputs the locations
of the joints after each iteration,~\cite{carreira2016human} outputs a
displacement vector for each joint. The displacement is applied and
the points are \textit{rendered} to heatmaps, which fed into the
network again for the next iteration. For the first iteration, a mean
pose is provided to the network.

The second approach to CNN based human pose estimation is to regress
heatmaps, as done
in~\cite{newell2016stacked,pfister2015flowing,zhou2016sparseness,pavlakos2017coarse,mehta2017vnect,zhao2018through}. In
these methods, an intensity value is regressed for each pixel, giving
a likelyhood of the joint in that pixel.
In~\cite{newell2016stacked}, a novel ``stacked hourglass'' network is
proposed. This architecture gradually reduces and increases the
spatial resolution while combining features from different
scales. Combining these features from different scales allows the
network to produce better predictions as both global and local context
is available to the last few layers of the network. A loss function is
used between each hourglass, which allows the network to refine and
reevaluate the pose as it passes through the network (similar to idea
of iterative methods,
e.g.~\cite{toshev2014deeppose,carreira2016human}). This architecture
in particular, despite intended for human pose estimation, has
strongly influenced our work on 3D reconstruction, discussed further
in Chapters~\ref{chapter:face} and~\ref{chapter:human}. A 3D heatmap
is regressed in~\cite{pavlakos2017coarse}, which is a similar idea to
our own 3D reconstruction, but for pose, not geometry. A part based
heatmap regression method is presented in~\cite{mehta2017vnect}.

In~\cite{pfister2015flowing}, video is used
instead of a single image. The method accepts an arbitrary number of
neighbouring frames for each time $t$. Features between frames are
combined using what they call ``Spatial Fusion Layers''. For each
frame at time $t$, a pose heatmap. Optical flow is used from the
neighbouring frames to time $t$ to refine the prediction at. Finally,
in~\cite{zhao2018through}, the authors present their method for
\textit{seeing through walls}, by using a standard CNN trained on 2D
radio frequency heatmaps, for both horizontal and vertical
polarizations.



\section{Human Body Reconstruction}

Many human reconstruction methods estimate the geometry from one or
more images. For
example,~\cite{balan2007detailed,grest2005human,guan2009estimating}
fit a model based on a single RGB or grey scale image. In particular,
\cite{grest2005human} fit a skeleton model to the image by estimating
the scale and pose of each body part
separately. In~\cite{guan2009estimating}, they fit a shape model
initialised by a user clicking on separate body parts, assisted by a
segmentation mask. Another shape model based approach is proposed
in~\cite{balan2007detailed}, using the SCRAPE
model~\cite{anguelov2005scape}, which is fitted with a stochastic
optimisation step. A general shape fitting method for reconstruction
is proposed in~\cite{chen2010inferring}, where two Gaussian models are
used - one for shape and one for pose, by solving non-linear
optimisation problems. The authors demonstrate this method on human
bodies and sharks. In~\cite{jiang20103d}, a single image and its
corresponding landmarks are used to lookup a similar human pose using
a kd-tree, containing about 4 million examples. A method intended for
multi-instance model fitting from a single image is described
in~\cite{Zanfir_2018_CVPR}.

Several methods aim to estimate the 3D geometry using only the
landmarks extracted via human pose
estimation~\cite{bogo2016smplify,ramakrishna2012reconstructing}. Particularly,
SMPLify~\cite{bogo2016smplify} (which uses the SMPL
model~\cite{loper2015smpl}), which was extended to also include
further guidance from an segmentation mask
in~\cite{varol2017learning}. However, such an approach will never be
able to capable of regressing finer details, unless information from
the image is also captured.

Aside from SCRAPE~\cite{anguelov2005scape} and
SMPL~\cite{loper2015smpl}, mentioned earlier, Dyna, the shape model
capable of capturing large variations in body shape is presented
in~\cite{Dyna:SIGGRAPH:2015}, but without an accompanying fitting
method from a single image. A very recent shape model called Total
Capture~\cite{Joo_2018_CVPR} captures many aspects of the body which
are typically ignored by other shape models, including the face and
hands.

The work we present in Chapter~\ref{chapter:human} is different from
all of the aforementioned methods in that we do not regress parameters
for a shape model, nor do we regress the vertices directly. Further
more, our method skips the model generation step entirely, which
avoids the need to find dense correspondence between all training
examples. Instead, we constrain the problem to the spatial domain, and
directly regress the 3D structure using spatial convolutions using a
CNN, via a volumetric representation from which the full 3D geometry
can be recovered.


\section{Semantic Segmentation}

Semantic segmentation is the process by which pixels of an image are
labelled by the objects contained in the image. For example, all dogs
may be labelled as red, while cats may be labelled as green. In order
for this to work, context is required, both locally and
globally. Global context tells us what is in the image and roughly
where a particular object may be located. Local context then refines
this global information, defining boundaries between classes with
greater precision.

Classical approaches to semantic segmentation involved first
segmenting the image using statistical models, followed by
individually classifying the segmented objects. See for
example~\cite{arbelaez2012semantic,carreira2012semantic}, which use a
Conditional Random Field (CRF) to separate the image. However, these
methods generally struggle to adequately incorporate high and low
level feature descriptors necessary for accurate segmentation and
labelling.

CNNs drastically changed the landscape of semantic segmentation. The
first work producing satisfactory semantic segmentation was the Fully
Convolutional Network (FCN) described in~\cite{long2015fully}, which
is based on VGG-16~\cite{simonyan2014vgg}. In this work, information
is forked from different layers throughout the network. Each fork is
used to produce an intensity map for all known class (initially 59,
targeted at the PASCAL VOC 2010
dataset~\cite{everingham2010pascal}). Forks earlier on in the network
provide very detailed but noisy predictions about the location of
classes, where as the latter forks are have very little noise, but can
only approximate the location of a class. A diagram of the structure
of FCN is shown in Figure~\ref{fig:fcn}. FCN was not intended to be
trained from scratch. Instead, training was initialised from the
parameters of VGG-16, once the network had been trained on the
ImageNet dataset~\cite{krizhevsky2012imagenet} for the task of
classification. The parameters in the last few layers of the network
were reshaped from linear layers into a $1\times 1$ convolution. An
interesting side effect of which, is the ability for the network to
accept inputs of arbitrary sizes. The size of the input no longer
dictates the number of parameters in the linear layers. Due to its
simplicity and (at the time) state of the art results, it is no
surprise the ideas behind FCN have become the standard for modern
semantic segmentation methods.

Still, one of the limitations of FCN is that the resolution of its
predictions are quite low. As such, several methods have proposed
extensions to FCN to compensate for this limitation. The most common
addition has been to add a CRF to the output of the network, to
provide further refinement. The work of \cite{chen2015semantic}
first upsamples the predicted scores using bilinear interpolation
and then refines the output by applying a dense CRF. The method of
\cite{zheng2015conditional} performs recurrent end-to-end training of
the FCN and the dense CRF. Finally, the work in \cite{noh2015learning}
employs learnt deconvolution layers, as opposed to fixing the
parameters with an interpolation filter (as in FCN). These filters
learn to reconstruct the object's mask, in addition to upsampling its
input.

In the past year or so, the Encoder-Decode network architecture has
gained a great deal of popularity. This architecture reduces the
spatial dimensionality of the feature maps as they are passed through
the \textit{encoder}. These features are then upsampled again using a
\textit{decoder} part of the network, while being recombined with
lower level information, much like in FCN~\cite{long2015fully}. This
architecture has also been referred to as an hourglass network. This
name was first used in~\cite{newell2016stacked}, and performed human
pose estimation with great precision. This level of precision is due
to the being suited to combining features from the high and low parts
of the network efficiently (i.e. combining both global and local
context). As such, similar architectures have been used for semantic
segmentation problems. For example,
SegNet~\cite{badrinarayanan2017segnet} uses an Encoder-Decoder
architecture to perform semantic segmentation on 11 classes for
autonomous driving in real time.

Architectures aside, a main drawback of this supervised approach to
semantic segmentation is producing large quantities of detailed
groundtruth semantic masks. There are several large datasets already
available, such as PASCAL VOC~\cite{everingham2010pascal}, Microsoft
COCO~\cite{lin2014microsoft} and
Cityscapes~\cite{cordts2016cityscapes}. All of these datasets are
fairly constrained in terms of the number of known
classes. Respectively, these datasets have 59, 183 and 30 known
classes. This is problematic when it comes to making semantic
segmentation a viable tool for general semantic segmentation
\textit{in-the-wild}. As such, numerous approaches have been devised
to train these models in ways requiring less human
intervention. In~\cite{richter2016playing}, hooks are added to
visually detailed computer games, to allow automatic generation of
instance aware semantic segmentation masks. A drawback of this is that
most games are far from realistic enough to work on real images. As
such, models trained on this data will also require domain adaptation
- which is an entire research area in itself. In~\cite{bearman2016s},
a weakly supervised approach is presented, where groundtruth semantic
segmentation masks are avoided entirely. Instead, the method learns
the semantic segmentation task from just a single point on each
object.

Another approach is described in~\cite{hu2018learning}, where the
training set is formed of images where all classes are annotated with
bounding boxes. However, only some classes have groundtruth masks. The
network is trained to produce both bounding boxes and semantic
segmentation masks simultaneously by learning a mapping between the
visual embedding from the bounding box feature extractors, to the
semantic segmentation feature extractors. They refer to this approach
as \textit{partially} supervised, as some groundtruth data is
required. Additionally, the method has been shown to be able to
segment up to 3000 classes - far higher than~\cite{long2015fully},
which was trained on the 59 classes from VOC PASCAL
2010~\cite{everingham2010pascal}.

The primary focus of this thesis is not semantic
segmentation. However, in Chapter~\ref{chapter:seg}, we describe our
method for facial part segmentation, in which each component of the
face (such as eyes, lips, nose, etc) are assigned a pixel wise
label. Deep learning based approaches to semantic segmentation,
particularly the fully convolutional approach described in
~\cite{long2015fully}, have been heavily influential in our work on 3D
reconstruction due to the spatial nature of our approach.

% \begin{figure}
%   \centering
%   \begin{tikzpicture}[scale=0.9]
%   \networkLayer{3.0}{0.48}{0}{0.0}{color=green!30}{96}
%   \networkLayer{1.5}{1.28}{0.48}{0.0}{color=green!30}{256}
%   \networkLayer{0.75}{1.92}{1.76}{0.0}{color=green!30}{384}
%   \networkLayer{0.75}{1.92}{3.68}{0.0}{color=green!30}{384}
%   \networkLayer{0.75}{1.28}{5.60}{0.0}{color=green!30}{256}
%   \networkLayer{0.37}{2.05}{6.88}{0.0}{color=green!30}{4096}
%   \networkLayer{0.37}{2.05}{8.93}{0.0}{color=green!30}{4096}
%   \networkLayer{0.37}{0.5}{10.98}{0.0}{color=green!30}{1000}

%   \networkLayer{0.75}{0.5}{12}{0.0}{color=red!30}{$\times 2$}
%   \networkLayer{1.5}{0.5}{13}{0.0}{color=red!30}{$\times 2$}
%   \networkLayer{3}{0.5}{14.5}{0.0}{color=red!30}{$\times 2$}

%   \draw [>->] (1.92,1.75) -- (1.92,2.5) -- (13.5,2.5);
%   \draw [>->] (6.88,1) -- (6.88,1.25) -- (12.5,1.25);

%   \draw (13.5,3) circle [radius=0.3] node {$+$};
%   \draw (12.5,1.75) circle [radius=0.3] node {$+$};
% \end{tikzpicture}
% \caption[The VGG-16 network]{Basic VGG-16~\cite{simonyan2014vgg}
%   network with modifications added from
%   FCN~\cite{long2015fully}. Information from lower levels of the
%   network summed to upsampled output of the tailing end of the
%   network. The number of features is shown below each convolution.}
% \label{fig:background:fcn}
% \end{figure}

\subsection{Part Segmentation}

There have been also a few works that
extend semantic segmentation to part segmentation with perhaps the
most well-known being the Shape Boltzman Machine
\cite{eslami2012generative,eslami2014shape}. This work has been
recently extended to incorporate CNN refined by CRF features (as in
\cite{chen2015semantic}) in \cite{tsogkas2015deep}. Note that this
work aims to refine the CNN output by applying a Restricted Boltzmann
Machine on top of it and does not make use of pose information as
provided by landmarks. In contrast, we propose an enhanced CNN
architecture which is landmark-guided, can be trained end-to-end and
yields large performance improvement without the need of further
refinement.


\subsection{Facial Part Segmentation}

One of the first face segmentation methods
prior to deep learning is known as LabelFaces
\cite{warrell2009labelfaces} which is based on patch classification
and further refinement via a hierarchical face model. Another
hierarchical approach to face segmentation based on Restricted
Boltzmann Machines was proposed in \cite{luo2012hierarchical}. More
recently, a multi-objective CNN has been shown to perform well for the
task of face segmentation in \cite{liu2015multi}. The method is based
on a CRF the unary and pairwise potentials of which are learnt via a
CNN. Softmax loss is used for the segmentation masks, and a logistic
loss is used to learn the edges. Additionally, the network makes use
of a non-parametric segmentation prior which is obtained as follows:
first facial landmarks on the test image are detected and then all
training images with most similar shapes are used to calculate an
average segmentation mask. This mask is finally used to augment
RGB. This segmentation mask might be blurry, does not encode pose
information and results in little performance improvement.











%%% Local Variables:
%%% TeX-master: "../thesis"
%%% End:
