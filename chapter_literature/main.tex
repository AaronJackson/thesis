\chapter{Literature Review}
\label{chapter:literature}

This section attempts to provide a comprehensive literature review of
the works relevant to this thesis. This thesis focuses primarily on
the 3D reconstruction of human faces, but includes an extension which
shows our method working on full human bodies. Methods for 3D
reconstruction of the face and body often depend on methods for first
extracting landmarks. As such, an overview of literature relating to
face alignment and human pose estimation is given in this literature
review, providing some context for the works relating to 3D face and
human body reconstruction.


\section{Face Reconstruction}
\label{chap:literature:sec:face_recon}

% 3D face reconstruction is the process of estimating the 3D geometry of
% a face. This is typically from one or more images, but methods relying
% on video, depth data, and landmarks also exist. %% More of an
% %% introduction required here.

The literature on 3D face reconstruction typically falls into several
categories. However, there is often also a lot of interaction or
intersection between multiple techniques in an attempt to garner
better results.

%%%%%%%%%%%%%%% 3DMM
\subsection{3D Morphable Models (3DMM)}
% Arguably the most popular approach to estimating the 3D geometry of
% the human face is using 3D Morphable Models (3DMM). These models use a
% small number of parameters to adjust facial properties such as shape,
% pose and expression. There are many methods based on this approach,
% including~\cite{jourabloo2016large,huber2016multiresolution,zhu2016face,liu2016joint,tran2018extreme,jiang20183d,jiang2018pose}.

Arguably the most popular approach to estimating the 3D geometry of
the human face is using 3D Morphable Models (3DMM). A model $s$
consist of a 3D mesh of $N$ vertices, defined as a $3N$ vector:
%
\begin{equation}
  s = [x_1, y_1, z_1, \dots, x_N, y_N, z_N]
\end{equation}
%
This model is constructed by finding the dense correspondence between
all vertices. This ensures that any particular vertex (e.g. the tip of
the nose) has the same index in all meshes. Additionally, it ensures
that all meshes have the same number of vertices. These meshes must
then be brought into a shape space, which ensures that the meshes are
aligned \textit{somewhere} against an unknown mean shape and is
performed by applying Generalised Procrustes Analysis. Principal
Component Analysis (PCA) is then performed which results in
$\{\boldsymbol{\overline{s}}_{id}, \boldsymbol{U}_{id},
\Sigma_{id}\}$, where $\boldsymbol{\overline{s}}_{id}$ is a vector
containing a mean shape, $\boldsymbol{U}_{id}$ is the first $n$
principal components for each vertex, and hence has the shape
$3N \times n$. Finally, $\Sigma_{id}$ is a diagonal matrix containing
the standard deviations of each principal component. Finally, there
are the shape parameters $\boldsymbol{p} = [p_1, \dots, p_n]$, which
can be used to generate a new face $\mathcal{S}$~\cite{booth20183d}:
%
\begin{equation}
  \mathcal{S}(\boldsymbol{p}) = \boldsymbol{\overline{s}}_{id}
  + \boldsymbol{U}_{id}\Sigma_{id}\boldsymbol{p}
\end{equation}
%
There are many methods based on this approach,
including~\cite{jourabloo2016large,huber2016multiresolution,zhu2016face,liu2016joint,tran2018extreme,jiang20183d,jiang2018pose}. In
particular,~\cite{jourabloo2016large} use a cascade of CNNs, which
first regress the camera projection, followed by the pose, followed by
shape and expression. Additionally,~\cite{jourabloo2016large} extract
features from their model fitting which can assist in facial landmark
localisation. A method capable of working with models of different
resolutions is proposed in~\cite{huber2016multiresolution}, along with
several models of different resolution, ranging from roughly 3500 to
30,000 vertices. This method does not use a CNN, instead regressing
parameters for shape and pose using only the landmarks. This is done
by iteratively minimising a cost function which evaluates the fitting.

Constraining the pose, shape and (optionally) appearance into a small
number of parameters has the undesirable side effect of being unable
to capture finer details, such as wrinkles and bumps. This is due to
the small number of parameters used and can be seen in many methods,
for example~\cite{richardson2016learning}, where the 3DMM fitting
stage is only used to extract a coarse
shape. In~\cite{sela2017unrestricted}, a CNN is used for 3DMM fitting,
followed by a refinement step which iteratively improves the finer
details. However, during this iterative refinement, the correspondence
between vertices, as well as the scale and spatial location of the
fitting is lost, which disregards one of the main advantages of 3DMM
based approaches. Another iterative approach attempting to
\textit{also} recover both the 3D geometry and facial landmarks is
described in~\cite{liu2016joint}. For initialisation, the method
passes the image and a mean shape from the facial landmarks. This mean
shape is generally generated by first aligning with a method such as
Procrustes, followed by taking the average location for each
point. These landmarks are gradually refined while learning a 3D to 2D
mapping, which updates the 3D face shape. Unlike our own method
(described in Chapter~\ref{chapter:face}),~\cite{liu2016joint} does
not estimate the facial pose, and so always produces a frontal
reconstruction. In many cases, the pose is an important component, and
as such, a separate method, likely based on the facial landmarks,
would have to be used to find the transformation for the 3D model.

A partially synthetic approach to 3D face reconstruction (indirectly
for face alignment) is proposed in~\cite{zhu2016face}. The method used
a single CNN that is iteratively applied to estimate the model
parameters using as input the 2D image and 3D shape and pose
representation from the previous iteration. The network consists of
four convolution layers and two fully connected layers. The first two
convolutions share parameters for memory efficient feature
extraction. This shape and pose representation can best be summarised
as a depth map of the fitting generated by parameters from the
previous iteration. The training data is augmented by artificially
rotating faces between $\{-90, 90\}$ degrees using groundtruth
parameters for a frontal fitting. One of the byproducts of this work
is a large dataset consisting of large pose synthetically rotated
faces, each with a corresponding 3D mesh.

%%%%%%%%%%%%%%% 3DMM + Synthetic
\subsection{Synthetic Training Data for 3DMM}
The data required for training 3D reconstruction methods typically
consists of an image with a corresponding 3D scan. Such pairs can be
difficult to obtain. \cite{richardson20163d} alleviate this issue by
generating synthetic training data, comprising of renderings of
randomly generated 3DMM with the corresponding mesh. Faces are
rendered under random lighting conditions and pose. An iterative
fitting method is then used during inference, optimising both geometry
and lighting to ensure a good match with the given image. Synthetic
data was also used for training a general purpose 3D reconstruction
method~\cite{li2015joint}. Methods trained using synthetic training
data often fail on real or ``in-the-wild'' images due to unrealistic
renderings or constrained lighting.



%%%%%%%%%%%%%%% Shape from Shading
\subsection{Shape from Shading}
A method which performs 3D face reconstruction from video is presented
in~\cite{suwajanakorn2014total}, which can also work from a set of
static images. The method combines their proposed 3D flow estimation
algorithm with a Shape from Shading (SfS) method. The method first
estimates the facial pose, which is used to rotate an average face
shape to the correct pose. For each frame, 3D flow updates the mesh,
which both refines the detail, and updates aspects such as expression
and pose. Another Shape from Shading approach is proposed
in~\cite{jiang20183d}, which first aligns a coarse facial model using
2D landmark detections. This coarse model is then \textit{corrected}
using the smaller eigenvalues of a Laplacian eigenfunction, which
results in small adjustments to the coarse geometry.  medium scale
features from the image. Finally, the shape from shading step refines
the mesh and introduces finer details.

%%%%%%%%%%%%%%% Multiview Approaches
\subsection{Multiview Reconstruction and Structure from Motion}

Several methods exist which rely on multiple images to reconstruct the
3D geometry of the face. These again fall into two more
categories. These are know as Multiview~\cite{mayo20093d,dou2018multi}
or Structure from Motion
(SfM)~\cite{dou2018multi,dai2018coarse,Piotraschke_2016_CVPR} based
methods, although again, there is some intersection between the two.

A keypoint based approach is presented in~\cite{mayo20093d}, where
SIFT is first used to extract dense features from each image. These
are then aligned using an Iterative Closest Point (ICP) algorithm to
build a point cloud of a face, from which a surface could be extracted
later. However, due to the simplicity of this approach, and the
limitations of ICP, this method will not account for changes in facial
expression between images. The work of~\cite{dou2018multi} proposes to
first pass each image through a CNN to extract feature maps, which are
then passed to a recurrent neural network (RNN) to regress parameters
for a 3DMM. The result is altered with each frame, making it a
suitable method for videos, where facial expression changes over time.

A two stage (coarse to fine) approach is presented
in~\cite{dai2018coarse}, where first a sparse set of landmarks are
first estimated, which are then used to estimate pose to assist in
fitting a dense set of points to the face to create a coarse
reconstruction. During this coarse reconstruction step, 14 images from
specific poses are extracted, which are used to refine the model and
extract details. Finally, in~\cite{Piotraschke_2016_CVPR}, a method is
proposed which reconstructs individual 3D shapes from multiple single
images of a face. The quality of each shape is evaluated. The best of
these shapes are them combined into a single 3D model. The aim of
doing this is to better handle variations in pose, lighting,
occlusions and facial expressions.

\subsection{Depth Estimation}

Depth estimation refers to the problem of estimating how far away each
pixel of an image would be, either relative to the other objects in
the image, or from the camera (i.e. real world space). Depth
estimation is typically applied to problems such as room
reconstruction, where only distances are required and not the full
geometry of every object in the room. Despite this, there are a couple
of methods which attempt to reconstruct the face by estimating the
depth from an image~\cite{sun2011depth,sun2013depth}. In~\cite{sun2011depth}, a
constrained Independent Component Analysis (ICA) model is used to
analyse multiple facial images of the same subject, from which the 3D
geometry is obtained. An almost identical method is presented
in~\cite{sun2013depth}, except instead of using an ICA model, a
nonlinear Least-Squares model is used, offering a small performance
improvement over~\cite{sun2011depth}.


%%%%%%%%%%%%%%% Our Method
\subsection{Where does our work fit in?}

In this section on 3D face reconstruction we have discussed several
different general approaches to reconstructing the shape of a person's
face. However, our method for 3D face reconstruction, described in
Chapter~\ref{chapter:face} is very different to the aforementioned
methods. First, our method does not rely on a 3D shape model for
either the pre-processing or ``fitting'' procedure, instead we rely on
a volumetric representation. This avoids the requirement estimate
correspondence between vertices, which is a challenging problem in
itself. Second, our method is direct, producing a volumetric
representation of the facial geometry in just a single step. Third,
unlike~\cite{tran2018extreme} and multiview approaches, we do not lose
any pose or scale information, and show potential for detailed
reconstruction, demonstrated in Chapter~\ref{chapter:human} on human
bodies.




\section{Human Body Reconstruction}

Human body reconstruction has a wide variety of applications, from
motion capture to trying on clothing in virtual environments. This
problem involves estimating the geometry for the full body, sometimes
with clothing, of all limbs and extremities. Many human reconstruction
methods estimate the geometry from one or more images. For
example,~\cite{balan2007detailed,grest2005human,guan2009estimating}
fit a model based on a single RGB or grey scale image. In particular,
\cite{grest2005human} fit a skeleton model to the image by estimating
the scale and pose of each body part
separately. In~\cite{guan2009estimating}, they fit a shape model
initialised by a user clicking on separate body parts, assisted by a
segmentation mask. Another shape model based approach is proposed
in~\cite{balan2007detailed}, using the SCRAPE
model~\cite{anguelov2005scape}, which is fitted with a stochastic
optimisation step. A general shape fitting method for reconstruction
is proposed in~\cite{chen2010inferring}, where two Gaussian models are
used - one for shape and one for pose, by solving non-linear
optimisation problems. The authors demonstrate this method on human
bodies and sharks. In~\cite{jiang20103d}, a single image and its
corresponding landmarks are used to lookup a similar human pose using
a kd-tree, containing about 4 million examples. A method intended for
multi-instance model fitting from a single image is described
in~\cite{Zanfir_2018_CVPR}.

Several methods aim to estimate the 3D geometry using only the
landmarks extracted via human pose
estimation~\cite{bogo2016smplify,ramakrishna2012reconstructing}. Particularly,
SMPLify~\cite{bogo2016smplify} (which uses the SMPL
model~\cite{loper2015smpl}), which was extended to also include
further guidance from an segmentation mask
in~\cite{varol2017learning}. However, such an approach will never be
able to capable of regressing finer details, unless information from
the image is also captured.

Aside from SCRAPE~\cite{anguelov2005scape} and
SMPL~\cite{loper2015smpl}, mentioned earlier, Dyna, the shape model
capable of capturing large variations in body shape is presented
in~\cite{Dyna:SIGGRAPH:2015}, but without an accompanying fitting
method from a single image. A very recent shape model called Total
Capture~\cite{Joo_2018_CVPR} captures many aspects of the body which
are typically ignored by other shape models, including the face and
hands.

The work we present in Chapter~\ref{chapter:human} is different from
all of the aforementioned methods in that we do not regress parameters
for a shape model, nor do we regress the vertices directly. Further
more, our method skips the model generation step entirely, which
avoids the need to find dense correspondence between all training
examples. Instead, we constrain the problem to the spatial domain, and
directly regress the 3D structure using spatial convolutions using a
CNN, via a volumetric representation from which the full 3D geometry
can be recovered.

\section{Landmark Detection}

Both face alignment and human pose estimation can be considered
special cases of landmark detection. The work presented in this thesis
optionally uses landmarks to guide our 3D reconstruction method, which
offers a slight performance improvement, although really shows its
effectiveness on difficult poses.

\subsection{Face Alignment}

Face alignment is the process of detecting a set of facial points to a
facial image. Face alignment is a prerequisite to many 3D face
reconstruction methods - in fact, methods such
as~\cite{huber2016multiresolution} depend only on facial landmarks,
discarding the image entirely.

% Our own methods optionally makes use of these landmarks to assist in
% the 3D reconstruction process. These are discussed in detail in
% Chapters~\ref{chapter:face} and~\ref{chapter:seg}.

Until recently, the highest performing methods for face alignment were
based on a cascaded regression approach. These methods estimate the
location of facial landmarks by applying a sequence of regressors. The
input to these regressors is typically SIFT~\cite{lowe2004distinctive}
and other hand selected features. These regressors are applied in a
cascaded fashion where the input to regressor $k$ is the estimate from
regressor $k-1$. More details of such approaches to face alignment can
be found
in~\cite{dollar2010cascaded,sanchez16,cao2014facewarehouse,xiongsupervised,zhu2015face,tzimiropoulos2015project}.

The remainder of this section will focus on deep learning based
approaches to face alignment. Deep convolutional networks have proven
to be an excellent choice for this kind of problem. One of the primary
reasons for this is due to the way global and local context can be
incorporated to provide both accuracy and precision.

% Similarly in~\cite{bulat2016two}, the location
% of landmarks in 3D are regressed and then optionally encoded as
% intensity heatmaps . If desired, the process can be
% stopped here. Optionally, this stack of heatmaps is passed through a
% second network, this time for regression, accompanied by the original
% input image. 

Several approaches directly regress the locations of the landmarks
using a tail of linear layers. For example, a CNN cascade is presented
in~\cite{sun2013deep} to directly produce the landmark locations. The
output of this second stage is 68 $(x, y, z)$ landmarks. A multitask
approach is proposed in~\cite{zhang2014facial}, regressing the
location of the facial landmarks while performing facial attribute
classification.

Many CNN based alignment methods estimate the facial landmarks by
regressing a 2D Gaussian located at each landmark
location~\cite{bulat2016two,bulat2017far,mahpod2018facial,kowalski2017deep,merget2018robust}.
This spatial approach is commonly referred to as heatmap
regression. One transitioning approach for 3D face alignment is
described in~\cite{bulat2016two}, where 2D landmarks are regressed as
heatmaps. These heatmaps are then passed through a second network,
performing regression to obtain 3D $x,y,z$ landmarks. This was
extended in~\cite{bulat2017far}, where the second regressor is
replaced with a CNN producing a 3D heatmap representation. A similar
heatmap based approach was used in a multi-stage approach described
in~\cite{kowalski2017deep}, where the landmark detections are refined
after each stage. Between each stage, the current landmark prediction
and some generated features are passed again through the network. At
the beginning of each stage, the inputs are transformed using the
landmarks to make the problem less challenging.

An indirect approach is discussed in~\cite{zhu2016face}, where a 3DMM
is first fitted, from which 3D landmarks are taken. This work is
discussed in further detail in
Section~\ref{chap:literature:sec:face_recon} as it is performing 3D
reconstruction, but for the purposes of face alignment. A similar idea
is proposed in~\cite{jourabloo2016large}, where a dense 3D model is
fit to the face first.

\subsection{Human Pose Estimation}

% While this thesis does not focus on pose estimation, in
% Chapter~\ref{chapter:human}, we present our method for 3D human body
% reconstruction as an extension to our 3D face reconstruction
% work. Human pose estimation is optionally used as guidance for this
% method in a similar fashion to face alignment being used as guidance
% to 3D face reconstruction.

Modern approaches to estimating human pose are based on methods
employing CNNs. These methods generally fall into one of two
categories. The first is to directly regress the coordinates of the
joints using an L2 (or similar) loss, see for
example~\cite{li20143d,park20163d,tekin2016structured,tekin2016direct,zhou2016deep,chen2016synthesizing,ghezelghieh2016learning,toshev2014deeppose,tompson2014joint}. In
particular,~\cite{park20163d} estimates the 3D pose by combining the
2D predictions with image features. An autoencoder is employed
in~\cite{tekin2016structured} to constrain pose to something
plausible. Similarly,~\cite{zhou2016deep} have the same goal but
achieve this by using a kinematic model. Synthetic training data is
used for the full training procedure in~\cite{chen2016synthesizing},
to ensure that the network is trained with accurate data. However,
in~\cite{ghezelghieh2016learning}, they only augment their existing
training set with synthetic data. ~\cite{toshev2014deeppose} propose
an iterative method, in which images are first passed through an
initalisation CNN. The output from this is then combined again with
the input image and passed through a second CNN multiple times
iteratively.  In~\cite{tompson2014joint}, a multi-resolution CNN is
trained jointly with a Markov Random Field, helping to refine the
prediction from the CNN by constraining pose. An iterative method is
presented in~\cite{carreira2016human}. However,
unlike~\cite{toshev2014deeppose}, which directly outputs the locations
of the joints after each iteration,~\cite{carreira2016human} outputs a
displacement vector for each joint. The displacement is applied and
the points are \textit{rendered} to heatmaps, which fed into the
network again for the next iteration. For the first iteration, a mean
pose is provided to the network.


The second approach to CNN based human pose estimation is to regress
heatmaps, as done
in~\cite{newell2016stacked,pfister2015flowing,zhou2016sparseness,pavlakos2017coarse,mehta2017vnect,zhao2018through}. In
these methods, an intensity value is regressed for each pixel, giving
a likelyhood of the joint in that pixel.  In~\cite{newell2016stacked},
a novel ``Stacked Hourglass'' network is proposed.  This architecture
in particular, despite being intended for human pose estimation, has
strongly influenced our work on 3D reconstruction, which is discussed
further in Chapters~\ref{chapter:face} and~\ref{chapter:human}, as
such the remainder of this paragraph will serve to explain the general
idea of this work. Ignoring the stacking aspect for just a moment, the
``hourglass'' architecture, also called an Encoder-Decoder network,
gradually reduces and increases the spatial resolution while combining
features from different scales. Combining these features from
different scales allows the network to produce better predictions, as
both global (e.g. \textit{What's the main object in this image?}) and
local context (e.g. \textit{Where is the boundary between these two
  objects?}) is available to the last few layers of the
network. Stacking these hourglass modules together with a loss
function between each one allows the network to refine and reevaluate
the pose as it passes through the network. This is a similar idea to
the iterative methods described in
~\cite{toshev2014deeppose,carreira2016human}. In the case
of~\cite{newell2016stacked}, the loss function used for human pose
estimation is Mean Squared Error (MSE):
%
\begin{equation}
  \text{MSE} = \frac{1}{n} \sum^n_{i=1} (Y_i - \hat{Y}_i)^2,
\end{equation}
%
which as the name suggests takes the mean of the
squared error between elements of $Y$ and $\hat{Y}$. For their
experiments, performance is measured for several configurations of
stacking, 2, 4 or 8 hourglasses. In each case, reducing the number of
residual modules to compensate for parameter count. The performance
between each configuration after the last layer is fairly close, but
the 8 hourglass configuration beats the 4 hourglass configuration,
which also beats the two hourglass configuration.


A 3D heatmap is regressed in~\cite{pavlakos2017coarse}, which is a
similar idea to our own 3D reconstruction, but for pose, not
geometry. A part based heatmap regression method is presented
in~\cite{mehta2017vnect}. In~\cite{pfister2015flowing}, video is used
instead of a single image. The method accepts an arbitrary number of
neighbouring frames for each time $t$. Features between frames are
combined using what they call ``Spatial Fusion Layers''. For each
frame at time $t$, a pose heatmap. Optical flow is used from the
neighbouring frames to time $t$ to refine the prediction at. Finally,
in~\cite{zhao2018through}, the authors present their method for
\textit{seeing through walls}, by using a standard CNN trained on 2D
radio frequency heatmaps, for both horizontal and vertical
polarizations.








%%% Local Variables:
%%% TeX-master: "../thesis"
%%% End:
