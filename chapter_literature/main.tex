 \chapter{Literature Review}


This section attempts to provide a comprehensive literature review of
the works relevant to this thesis. This thesis focuses primarily on
the 3D reconstruction of human faces and bodies. Much of the
literature discussed in this chapter has an obvious
relevance. However, in the cases which are less obvious, connections
have attempted to be made with references to the relevant chapter and
section.

\section{Face Alignment}

Until recently, the highest performing methods for face alignment were
based on a cascaded regression approach. These methods estimate the
location of facial landmarks by applying a sequence of
regressors. Provided to these regressors is typically
SIFT~\cite{lowe2004distinctive} and other hand selected
features. These regressors are applied in a cascaded fashion where the
input to regressor $k$ is the estimate from regressor $k-1$. More
details of such approaches to face alignment can be found
in~\cite{sanchez16,cao2014facewarehouse,xiongsupervised,zhu2015face,tzimiropoulos2015project}.

The remainder of this section will focus on deep learning based
approaches to face alignment. A CNN casecade is presented
in~\cite{sun2013deep} to directly produce the landmark
locations. Similarly in~\cite{bulat2016two}, the authors propose a
method for 3D face alignment by first detecting the 2D landmarks using
heatmap regression. If desired, the process can be stopped
here. Optionally, this stack of heatmaps is passed through a second
network, this time for regression, accompanied by the original input
image. The output of this second stage is 68 $(x, y, z)$
landmarks. A multitask approach is proposed in~\cite{zhang2014facial},
regressing the location of the facial landmarks while performing
facial attribute classification.

Many CNN based alignment methods estimate the facial landmarks by
regressing a 2D Gaussian on top of each landmark
location~\cite{bulat2016two,bulat2017far,mahpod2018facial,kowalski2017deep,merget2018robust}.
This spatial approach is commonly referred to as heatmap
regression. One such example is the extension to~\cite{bulat2016two}
in~\cite{bulat2017far}, where the second regressor is replaced with a
CNN producing a 3D heatmap representation. A similar heatmap based
approach was used in a multi-stage approach described
in~\cite{kowalski2017deep}, where the landmark detections are refined
after each stage. Between each stage, the \textit{current} landmark
prediction and some generated features are passed. At the beginning of
each stage, the inputs are transformed using the landmarks to make the
problem less challenging.

An indirect approach is discussed in~\cite{zhu2016face}, where a 3DMM
is first fitted, from which 3D landmarks are taken. This work is
discussed in further detail in
Section~\ref{chap:literature:sec:face_recon}. A similar idea is
propsed in~\cite{jourabloo2016large}, where a dense 3D model is fit to
the face first.

Our work does not attempt to improve the \textit{state-of-the-art} in
face alignment. Instead, we optionally employ face alignment as a tool
for guidance in Chapters~\ref{chapter:seg} and~\ref{chapter:face},
which we show can improve the performance of these methods quite
significantly.


\section{Face Reconstruction}
\label{chap:literature:sec:face_recon}

3D face reconstruction is the process of estimating the 3D geometry of
a face. This is typically from one or more images, but methods relying
on video, depth data, and landmarks also exist. Arguably the most
popular approach to estimating the 3D geometry of the human face is
using 3D Morphable Models (3DMM). These models consist of a small
number of parameters which adjust facial properties such as shape,
pose and expression. Such methods
include~\cite{jourabloo2016large,huber2016multiresolution,zhu2016face,liu2016joint,tran2018extreme,jiang20183d,jiang2018pose}.

In particular,~\cite{jourabloo2016large} use a cascade of CNNs, which
first regress the camera projection, followed by the pose, followed by
shape and expression. Additionally,~\cite{jourabloo2016large} extract
features from their model fitting which can assist in facial landmark
localisation. A method capable of working with models of different
resolutions is proposed in~\cite{huber2016multiresolution}, along with
several models of different resolution, ranging from roughly 30,000 to
3500 vertices. This method does not use a CNN, instead regressing
parameters for shape and pose from the landmarks only, by minimising a
cost function which evaluates the fitting.

Constraining the pose, shape and (optionally) appearance into a small
number of parameters has the undesirable side effect of being unable
to capture finer details, such as wrinkles and
bumps. In~\cite{tran2018extreme}, a CNN is used for 3DMM fitting,
followed by a refinement step which iteratively improves the finer
details. However, during this iterative refinement, the correspondence
between vertices, as well as the scale and spatial location of the
fitting is lost. Another iterative approach attempting to
\textit{also} recover both the 3D geometry and facial landmarks is
described in~\cite{liu2016joint}. For initialisation, the method
passes the image and a mean set of facial landmarks. These landmarks
are gradually refined while learning a 3D to 2D mapping, which updates
the 3D face shape. Unlike the aforementioned methods, our own method
(described in Chapter~\ref{chapter:face})~\cite{liu2016joint} does not
estimate the facial pose, and as such always produces a frontal
reconstruction.

The data required for training 3D reconstruction methods typically
consists of an image with a corresponding 3D scan and can be difficult
to obtain. \cite{richardson20163d} avoid this issue by generating
synthetic training data, composed of renderings of randomly generated
3DMM with corresponding meshes. Faces are rendered under random
lighting conditions and projected only the image plane. An incremental
fitting method is then used during inferences. Synthetic data was also
used for training a general purpose 3D reconstruction
method~\cite{li2015joint}. Methods trained using synthetic training
data often fail on real or ``in-the-wild'' images due to unrealistic
or constrained lighting.

A partially synthetic approach to 3D face reconstruction (indirectly
for face alignment) is proposed in~\cite{zhu2016face}. The method used
a single CNN that is iteratively applied to estimate the model
parameters using as input the 2D image and 3D shape and pose
representation from the previous iteration. The network consists of
four convolution layers and two fully connected layers. The first two
convolutions share parameters for memory efficient feature
extraction. This shape and pose representation can best be summarised
as a depth map of the fitting generated by parameters from the
previous iteration. The training data is augmented by artificially
rotating faces between $\{-90, 90\}$ degrees using groundtruth
parameters for a frontal fitting.

A method which performs 3D reconstruction from video is presented
in~\cite{suwajanakorn2014total}, which can also work from a set of
static images. The method combines their proposed 3D flow estimation
algorithm with a Shape from Shading (SfS) method. The method first
estimates the facial pose, which is used to rotate an average face
shape to the correct pose. For each frame, 3D flow updates the mesh,
which both refines the detail, and updates aspects such as expression
and pose. Another Shape from Shading approach is proposed
in~\cite{jiang20183d}, which first aligns a coarse face reconstruction
using 2D landmark detections. This coarse model is then
\textit{corrected} using medium scale features from the
image. Finally, the shape from shading step refines the mesh and
introduces finer details.

Our method for 3D face reconstruction, described in
Chapter~\ref{chapter:face} is very different to the aforementioned
methods. First, our method does not rely on a 3D shape model for
either the pre-processing or ``fitting'' procedure, instead we rely on
a volumetric representation. This avoids the requirement to find
correspondence between vertices, which is a challenging problem in
itself. Second, our method is direct, producing a volumetric
representation of the facial geometry in just a single step. Third,
unlike~\cite{tran2018extreme} we do not lose any pose or scale
information, and show potential for detailed reconstruction,
demonstrated in Chapter~\ref{chapter:human} on human bodies.



\section{Human Pose Estimation}

While this thesis does not focus on pose estimation, in
Chapter~\ref{chapter:human}, we present our method for 3D human body
reconstruction. Human pose estimation is optionally used as guidance
for this method.

Almost all recent methods for human pose estimation have depended on
convolutional neural
networks~\cite{toshev2014deeppose,tompson2014joint,pfister2015flowing,newell2016stacked,carreira2016human,zhao2018through}. These
methods typically take a single image as input and as output produce
the landmarks. However,~\cite{pfister2015flowing} uses video
and~\cite{zhao2018through} monitors radio waves.

Commonly referred to as DeepPose,~\cite{toshev2014deeppose} proposes
an iterative method, in which images are first passed through an
initalisation CNN. The output from this is then combined again with
the input image and passed through a second CNN multiple times
iteratively. In~\cite{tompson2014joint}, a multi-resolution CNN is
trained jointly with a Markov Random Field, helping to refine the
prediction from the CNN by constraining pose.

In~\cite{newell2016stacked}, a novel ``stacked hourglass'' network is
proposed. This architecture gradually reduces and increases the
spatial resolution while combining features from different
scales. Combining these features from different scales allows the
network to produce better predictions as both global and local context
is available to the last few layers of the network. A loss function is
used between each hourglass, which allows the network to refine and
reevaluate the pose as it passes through the network (similar to idea
of iterative methods,
e.g.~\cite{toshev2014deeppose,carreira2016human}). The final output of
this network is 16 2D heatmaps, giving each pixel a likelihood value
for the location of the joint from which the joint locations can be
taken. This architecture in particular, despite intended for human
pose estimation, has strongly influenced our work on 3D
reconstruction, discussed further in Chapters~\ref{chapter:face}
and~\ref{chapter:human}.

Another iterative method is presented
in~\cite{carreira2016human}. However,
unlike~\cite{toshev2014deeppose}, which directly outputs the locations
of the joints after each iteration,~\cite{carreira2016human} outputs a
displacement vector for each joint. The displacement is applied and
the points are \textit{rendered} to heatmaps, which fed into the
network again for the next iteration. For the first iteration, a mean
pose is provided to the network.

In~\cite{pfister2015flowing}, video is used instead of a single
image. The method accepts an arbitrary number of neighbouring frames
for each time $t$. Features between frames are combined using what
they call ``Spatial Fusion Layers''. For each frame at time $t$, a
pose heatmap. Optical flow is used from the neighbouring frames to
time $t$ to refine the prediction at. Finally,
in~\cite{zhao2018through}, the authors present their method for
\textit{seeing through walls}, by using a standard CNN trained on 2D
radio frequency heatmaps, for both horizontal and vertical
polarizations.


\section{Human Body Reconstruction}




\section{Semantic Segmentation}

Semantic segmentation is the process by which pixels of an image are
labelled by the objects contained in the image. For example, all dogs
may be labelled as red, while cats may be labelled as green. In order
for this to work, context is required, both locally and
globally. Global context tells us what is in the image and roughly
where a particular object may be located. Local context then refines
this global information, defining boundaries between classes with
greater precision.

Classical approaches to semantic segmentation involved first
segmenting the image using statistical models, followed by
individually classifying the segmented objects. See for
example~\cite{arbelaez2012semantic,carreira2012semantic}, which use a
Conditional Random Field (CRF) to separate the image. However, these
methods generally struggle to adequately incorporate high and low
level feature descriptors necessary for accurate segmentation and
labelling.

CNNs drastically changed the landscape of semantic segmentation. The
first work producing satisfactory semantic segmentation was the Fully
Convolutional Network (FCN) described in~\cite{long2015fully}, which
is based on VGG-16~\cite{simonyan2014vgg}. In this work, information
is forked from different layers throughout the network. Each fork is
used to produce an intensity map for all known class (initially 59,
targeted at the PASCAL VOC 2010
dataset~\cite{everingham2010pascal}). Forks earlier on in the network
provide very detailed but noisy predictions about the location of
classes, where as the latter forks are have very little noise, but can
only approximate the location of a class. A diagram of the structure
of FCN is shown in Figure~\ref{fig:background:fcn}. FCN was not
intended to be trained from scratch. Instead, training was initialised
from the parameters of VGG-16, once the network had been trained on
the ImageNet dataset~\cite{krizhevsky2012imagenet} for the task of
classification. The parameters in the last few layers of the network
were reshaped from linear layers into a $1\times 1$ convolution. An
interesting side effect of which, is the ability for the network to
accept inputs of arbitrary sizes. The size of the input no longer
dictates the number of parameters in the linear layers. Due to its
simplicity and (at the time) state of the art results, it is no
surprise the ideas behind FCN have become the standard for modern
semantic segmentation methods.

Still, one of the limitations of FCN is that the resolution of its
predictions are quite low. As such, several methods have proposed
extensions to FCN to compensate for this limitation. The most common
addition has been to add a CRF to the output of the network, to
provide further refinement. The work of \cite{chen2015semantic}
first upsamples the predicted scores using bilinear interpolation
and then refines the output by applying a dense CRF. The method of
\cite{zheng2015conditional} performs recurrent end-to-end training of
the FCN and the dense CRF. Finally, the work in \cite{noh2015learning}
employs learnt deconvolution layers, as opposed to fixing the
parameters with an interpolation filter (as in FCN). These filters
learn to reconstruct the object's mask, in addition to upsampling its
input.

In the past year or so, the Encoder-Decode network architecture has
gained a great deal of popularity. This architecture reduces the
spatial dimensionality of the feature as they are passed through the
\textit{encoder}. These features are then upsampled again using a
\textit{decoder} part of the network, while being recombined with
lower level information, much like in FCN~\cite{long2015fully}. This
architecture has also been referred to as an hourglass network. This
name was first used in~\cite{newell2016stacked}, and performed human
pose estimation with great precision. This level of precision is due
to the being suited to combining features from the high and low parts
of the network efficiently (i.e. combining both global and local
context). As such, similar architectures have been used for semantic
segmentation problems. For example,
SegNet~\cite{badrinarayanan2017segnet} uses an Encoder-Decoder
architecture is used to perform semantic segmentation on 11 classes
for autonomous driving in real time.

Architectures aside, a main drawback of this supervised approach to
semantic segmentation is producing large quantities of detailed
groundtruth semantic masks. There are several large datasets already
available, such as PASCAL VOC~\cite{everingham2010pascal}, Microsoft
COCO~\cite{lin2014microsoft} and
Cityscapes~\cite{cordts2016cityscapes}. All of these datasets are
fairly constrained in terms of the number of known
classes. Respectively, these datasets have 59, 183 and 30 known
classes. This is problematic when it comes to making semantic
segmentation a viable tool for general semantic segmentation
\textit{in-the-wild}. As such, numerous approaches have been devised
to train these models in ways requiring less human
intervention. In~\cite{richter2016playing}, hooks are added to
visually detailed computer games, to allow automatic generation of
instance aware semantic segmentation masks. A drawback of this is that
most games are far from realistic enough to work on real images. As
such, models trained on this data will also require domain adaptation
- which is an entire research area in itself. In~\cite{bearman2016s},
a weakly supervised approach is presented, where groundtruth semantic
segmentation masks are avoided entirely. Instead, the method learns
the semantic segmentation task from just a single point on each
object.

Another approach is described in~\cite{hu2018learning}, where the
training set is formed of images where all classes are annotated with
bounding boxes. However, only some classes have groundtruth masks. The
network is trained to produce both bounding boxes and semantic
segmentation masks simultaneously by learning a mapping between the
visual embedding from the bounding box feature extractors, to the
semantic segmentation feature extractors. The paper describes this
working on 3000 segmentation classes. They refer to this approach as
\textit{partially} supervised, as some groundtruth data is
required. Additionally, the method has been shown to be able to
segment up to 3000 classes - far higher than~\cite{long2015fully},
which was trained on the 59 classes from VOC PASCAL
2010~\cite{everingham2010pascal}.

The primary focus of this thesis is not semantic
segmentation. However, in Chapter~\ref{chapter:seg}, we describe our
method for facial part segmentation, in which each component of the
face (such as eyes, lips, nose, etc) are assigned a pixel wise
label. Deep learning based approaches to semantic segmentation,
particularly the fully convolutional approach described in
~\cite{long2015fully}, have been heavily influential in our work on 3D
reconstruction.

\begin{figure}
  \centering
  \begin{tikzpicture}[scale=0.9]
  \networkLayer{3.0}{0.48}{0}{0.0}{color=green!30}{96}
  \networkLayer{1.5}{1.28}{0.48}{0.0}{color=green!30}{256}
  \networkLayer{0.75}{1.92}{1.76}{0.0}{color=green!30}{384}
  \networkLayer{0.75}{1.92}{3.68}{0.0}{color=green!30}{384}
  \networkLayer{0.75}{1.28}{5.60}{0.0}{color=green!30}{256}
  \networkLayer{0.37}{2.05}{6.88}{0.0}{color=green!30}{4096}
  \networkLayer{0.37}{2.05}{8.93}{0.0}{color=green!30}{4096}
  \networkLayer{0.37}{0.5}{10.98}{0.0}{color=green!30}{1000}

  \networkLayer{0.75}{0.5}{12}{0.0}{color=red!30}{$\times 2$}
  \networkLayer{1.5}{0.5}{13}{0.0}{color=red!30}{$\times 2$}
  \networkLayer{3}{0.5}{14.5}{0.0}{color=red!30}{$\times 2$}

  \draw [>->] (1.92,1.75) -- (1.92,2.5) -- (13.5,2.5);
  \draw [>->] (6.88,1) -- (6.88,1.25) -- (12.5,1.25);

  \draw (13.5,3) circle [radius=0.3] node {$+$};
  \draw (12.5,1.75) circle [radius=0.3] node {$+$};
\end{tikzpicture}
\caption[The VGG-16 network]{Basic VGG-16~\cite{simonyan2014vgg}
  network with modifications added from
  FCN~\cite{long2015fully}. Information from lower levels of the
  network summed to upsampled output of the tailing end of the
  network. The number of features is shown below each convolution.}
\label{fig:background:fcn}
\end{figure}

\subsection{Part Segmentation}

\subsection{Facial Part Segmentation}











%%% Local Variables:
%%% TeX-master: "../thesis"
%%% End:
